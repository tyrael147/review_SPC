{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.searcher import Searcher\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#authenticate, insert your API key in src/config.json\n",
    "sr = Searcher(\"src/config.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# keywords for each topics are defined\n",
    "# using constructors \"TITLE-ABS-KEY\" to search, title, abstract and keywords\n",
    "\n",
    "sustainability_q = ' TITLE-ABS-KEY(\"lca\" OR \"life cycle\" OR \"life-cycle\" OR \"sustainab*\" OR \"life cycle assessment\" OR \"environment*\") '\n",
    "resilience_q = ' TITLE-ABS-KEY(\"resilience\" OR \"disrupti*\" OR \"vulnerab*\" OR \"critical*\" OR \"robust*\") '\n",
    "complexity_q = ' TITLE-ABS-KEY(\"complexity\" OR \"agent-based\" OR \"agent based\" OR \"topolo*\" OR \"complex adaptive system\" OR \"networks\" OR \"network\" OR \"network analysis\" ) '\n",
    "supply_q = ' TITLE-ABS-KEY(\"supply chain\" OR \"supply network\" OR \"supply networks\" OR \"supply networks\" OR \"supply chains\") '\n",
    "restrictions = \" SRCTYPE(j) AND PUBYEAR AFT 2000\"\n",
    "separator = \" AND \""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fields_1 = [resilience_q, complexity_q, supply_q, restrictions]\n",
    "fields_2 = [sustainability_q, resilience_q, supply_q, restrictions]\n",
    "fields_3 = [sustainability_q, complexity_q, supply_q, restrictions]\n",
    "fields_4 = [sustainability_q, resilience_q, complexity_q, supply_q, restrictions]\n",
    "\n",
    "queries = [separator.join(field) for field in [fields_1, fields_2, fields_3, fields_4]]\n",
    "#\n",
    "for query in queries:\n",
    "    print(sr.get_search(query).tot_num_res)\n",
    "# Number of articles per query (requested on 08/03/2021)\n",
    "# 2983\n",
    "# 3596\n",
    "# 4717\n",
    "# 985"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_papers_dict = {}\n",
    "all_papers_df = pd.DataFrame()\n",
    "\n",
    "for idx, query in enumerate(queries):\n",
    "    query_dict, query_df = sr.search_query(query, get_all = True) #returns a dict and a df\n",
    "\n",
    "    all_papers_dict = {**all_papers_dict, **query_dict}\n",
    "    #discards duplicates\n",
    "    all_papers_df = pd.concat([all_papers_df, query_dict]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    #stores a pickle file to store the dict response that contains data and request metadata\n",
    "    pickle.dump(query_dict, open('data/query_dict_test'+str(idx)+'.p', 'wb'))\n",
    "    query_df.to_csv('data/query_df_'+str(idx)+'.csv', index=False)\n",
    "\n",
    "pickle.dump(all_papers_dict, open('data/all_papers'+'.p', 'wb'))\n",
    "all_papers_df.to_csv('data/all_papers_df'+'.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#This step uses the stored data to avoid making queries when experimentation is required\n",
    "# Merging dicts\n",
    "query_dict0 = pickle.load(open('data/query_dict_test0.p','rb'))\n",
    "query_dict1 = pickle.load(open('data/query_dict_test1.p','rb'))\n",
    "query_dict2 = pickle.load(open('data/query_dict_test2.p','rb'))\n",
    "query_dict3 = pickle.load(open('data/query_dict_test3.p','rb'))\n",
    "\n",
    "all_papers_dict = {**query_dict0, **query_dict1, **query_dict2, **query_dict3}\n",
    "\n",
    "#Merging df's\n",
    "df_0 = pd.read_csv('data/query_df_0.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "df_1 = pd.read_csv('data/query_df_1.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "df_2 = pd.read_csv('data/query_df_2.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "df_3 = pd.read_csv('data/query_df_3.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "all_papers_df = pd.concat([df_0, df_1, df_2, df_3]).drop_duplicates().reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def curate_for_cortext(_df):\n",
    "    df = _df.fillna(' ')\n",
    "    df['year'] = df['year'].astype('int16')\n",
    "    df['keywords'] = df['keywords'].apply(lambda kw: re.sub(' \\| ', ' *** ', kw))\n",
    "    return df\n",
    "#To use CorText we can replace nan with ' '\n",
    "all_papers_for_cortext = curate_for_cortext(all_papers_df)\n",
    "all_papers_for_cortext.to_csv('data/papers_for_cortext.csv', index=False)\n",
    "\n",
    "final_search_cortext = curate_for_cortext(df_3)\n",
    "final_search_cortext.to_csv('data/papers_for_cortext_final.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#importing from WOS\n",
    "# No API was available, manual exportation was performed.\n",
    "wos_0 = pd.read_csv('data/wos_0.csv')\n",
    "wos_1 = pd.read_csv('data/wos_1.csv')\n",
    "wos_2 = pd.read_csv('data/wos_2.csv')\n",
    "wos = pd.concat([wos_0,wos_1,wos_2])\n",
    "wos = wos[['Abstract', 'DOI', 'Source Title', 'Author Keywords','UT (Unique WOS ID)', 'Article Title', 'Publication Year']]\n",
    "wos.columns = final_search_cortext.columns\n",
    "wos['keywords'] = wos['keywords'].fillna(' ')\n",
    "wos['year'] = wos['year'].fillna(2021)\n",
    "wos['year'] = wos['year'].astype('int16')\n",
    "wos['keywords'] = wos['keywords'].apply(lambda kw: re.sub('; ', ' *** ', kw))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#merging WOS and SCOPUS\n",
    "full_final = pd.concat([final_search_cortext, wos]).drop_duplicates(subset=['doi']).reset_index(drop=True)\n",
    "full_final['journal'] = full_final['journal'].apply(lambda journal: journal.lower())\n",
    "full_final= full_final.sort_values(by=['year'], ascending=False)\n",
    "full_final.to_csv('data/wos_scopus_for_cortext.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}